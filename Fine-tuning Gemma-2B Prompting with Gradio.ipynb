{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1SSv6lzX3Byu50PooYogmiwHqf5PQN68E","timestamp":1712413359144},{"file_id":"1V3KHWuryEJMnIR2derqDE-m9A9OxqtqF","timestamp":1693813905257},{"file_id":"1SQmK0GYz34RGVlOnL5YMkdm7hXD6OjQT","timestamp":1693799934411}],"gpuType":"V100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"47b584cf4a35451282e117ee8ebe6c18":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ab061849f2044b195929ed1a9907141","IPY_MODEL_af4ffe62a81f4854b864b47df93a92b5","IPY_MODEL_c0ba233a565243eca85255d150753d9a"],"layout":"IPY_MODEL_333f1ff13fee4078aa7c0ae2d23ec7bb"}},"2ab061849f2044b195929ed1a9907141":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fd10dc272954f2e8aa2243dceec450d","placeholder":"​","style":"IPY_MODEL_3ecc05566e644e30a2d3ab8ef6608dde","value":"Loading checkpoint shards: 100%"}},"af4ffe62a81f4854b864b47df93a92b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_91412103c98d4460b5591da405d1eb04","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c663314a9c49436fbc6d7b02d56e25dc","value":2}},"c0ba233a565243eca85255d150753d9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfeb9d8bb8444c1e90fe34e460d1c08f","placeholder":"​","style":"IPY_MODEL_5e3ebed1f68a4314a9c936df184e998f","value":" 2/2 [00:04&lt;00:00,  2.05s/it]"}},"333f1ff13fee4078aa7c0ae2d23ec7bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fd10dc272954f2e8aa2243dceec450d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ecc05566e644e30a2d3ab8ef6608dde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91412103c98d4460b5591da405d1eb04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c663314a9c49436fbc6d7b02d56e25dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfeb9d8bb8444c1e90fe34e460d1c08f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e3ebed1f68a4314a9c936df184e998f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18a4db85e2234ec8b1fb88e499a7c2bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a22f4cafca984284808ac48ffd99b241","IPY_MODEL_51013517527a43e2bf06893a031b83fe","IPY_MODEL_2c96fcb457db403cbf081d0e0eff464e"],"layout":"IPY_MODEL_f79cfd0a60194188a8e04e1f0aa47e1f"}},"a22f4cafca984284808ac48ffd99b241":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8652175e344d41d3b40c0c93d6acf78c","placeholder":"​","style":"IPY_MODEL_7494f19ffeaa4fcb9dd1e27a3a3d529b","value":"Loading checkpoint shards: 100%"}},"51013517527a43e2bf06893a031b83fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95a6928dd57848bca19c91eec92041c3","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_edd610756c324aff89df79bbabb56362","value":3}},"2c96fcb457db403cbf081d0e0eff464e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21fb50c1655149cfa131a928c18eca40","placeholder":"​","style":"IPY_MODEL_78ccf2c08f3e4ccfbb393802a5f4203f","value":" 3/3 [00:04&lt;00:00,  1.34s/it]"}},"f79cfd0a60194188a8e04e1f0aa47e1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8652175e344d41d3b40c0c93d6acf78c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7494f19ffeaa4fcb9dd1e27a3a3d529b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95a6928dd57848bca19c91eec92041c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edd610756c324aff89df79bbabb56362":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21fb50c1655149cfa131a928c18eca40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78ccf2c08f3e4ccfbb393802a5f4203f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# 이 코랩파일은 유튜브 https://www.youtube.com/watch?v=lSBX-nMQ8cE 에 올라온 Gradio를 이용한 Llama2 프롬프팅 가이드 ipynb 파일을 일부 수정한 파일(코드)입니다.\n","\n","# 이 파일에서 수정된 부분은 다음과 같습니다.\n","# 1. road llm model 변경: Llama2 -> Fine-tuning Gemma-2B   (https://huggingface.co/taeseo06/Gemma-2b-PersonalizedFinetuning)\n","# 2. 모델을 불러오는 과정에서 GoogleDrive 를 연결하여 드라이브에 저장된 파인튜닝 모델, 토크나이저 직접 로드하는 방식 차용\n","# 3. 한국어 답변 및 일부 출력 오류를 해결하기 위해 generate(모델 생성)부분 일부 수정"],"metadata":{"id":"2flEO5TszBSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers torch accelerate"],"metadata":{"id":"aNTmMJIMYjiC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712413932577,"user_tz":-540,"elapsed":69756,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"27d49c4c-e5eb-44f8-ea03-d49229fd7e5c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Collecting accelerate\n","  Downloading accelerate-0.29.1-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["!pip install --upgrade gradio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFjx3o4HU_83","executionInfo":{"status":"ok","timestamp":1712413947609,"user_tz":-540,"elapsed":15035,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"5cf9f963-e5ae-48b0-f30f-f9e3bd73c57d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-4.25.0-py3-none-any.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.15.0 (from gradio)\n","  Downloading gradio_client-0.15.0-py3-none-any.whl (313 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.3.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.4)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.15.0->gradio) (2023.6.0)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.15.0->gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n","Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=b94a16b30661b3988a3cd691405b64da0e7bc0d3b77d8d8b74956b688bf7b366\n","  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n","Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.1 ffmpy-0.3.2 gradio-4.25.0 gradio-client-0.15.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.5 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n"]}]},{"cell_type":"code","source":["# import locale\n","# locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"],"metadata":{"id":"ktVvj4nuVcn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"_6AfgF3_arYL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712413982361,"user_tz":-540,"elapsed":34758,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"2456570b-8a48-433d-bcc6-ddd7bba13725"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) y\n","Token is valid (permission: read).\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["!huggingface-cli whoami"],"metadata":{"id":"6lD_oW1uavGp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712413985888,"user_tz":-540,"elapsed":624,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"327cd89e-c644-42de-ac57-58a2825b3bf1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["taeseo06\n"]}]},{"cell_type":"markdown","source":[" ---\n","## 모델, 토크나이저 로드\n","GoogleDrive를 연결하여 직접 드라이브에 접근해서 파인튜닝 된 모델과 토크나이저를 불러오는 방법과, HuggingFace Pipeline으로 모델을 로드하는 방법으로 나누었습니다.\n","\n","! 외부에서 시연 및 접근 할 때에는 GoogleDrive 연결이 아닌 HuggingFace 로드가 적합.\n"],"metadata":{"id":"xmJHSjx4abta"}},{"cell_type":"markdown","source":["### 1️⃣. GoogleDrive에서 직접적으로 모델, 토크나이저 로드"],"metadata":{"id":"knOakuQ42OCJ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSQ2ybug2W04","executionInfo":{"status":"ok","timestamp":1712414415161,"user_tz":-540,"elapsed":20376,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"f796acde-4109-48d9-adcf-d75a160c52d3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#@title TrainedModel 폴더에 저장된 파인튜닝 모델이름 -> 시도 횟수\n","trial = '3' #@param {type: \"string\"}\n"],"metadata":{"id":"sI6Hi_al24g3","executionInfo":{"status":"ok","timestamp":1712414543031,"user_tz":-540,"elapsed":1,"user":{"displayName":"문태서","userId":"03540612891035297255"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 현재 학습을 시도하는 횟수를 지정해서 각각의 경로마다 변경할 수 있게\n","Route = '/content/drive/MyDrive/Colab Notebooks/LLM 파인튜닝 프로젝트 (model-llama2)/llama2-PersonalizedFinetuning/TrainedModel/' + str(trial) + 'st' # 학습된 모델의 기본 루트\n","\n","%cd $Route"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zH28AV7I2mP5","executionInfo":{"status":"ok","timestamp":1712414559691,"user_tz":-540,"elapsed":4,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"1633e697-fe7b-4cf2-e0a6-959cfab9b933"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/LLM 파인튜닝 프로젝트 (model-llama2)/llama2-PersonalizedFinetuning/TrainedModel/3st\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model = AutoModelForCausalLM.from_pretrained(\"./merged_LLM_Model\")\n","tokenizer = AutoTokenizer.from_pretrained(\"./merged_LLM_Model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["47b584cf4a35451282e117ee8ebe6c18","2ab061849f2044b195929ed1a9907141","af4ffe62a81f4854b864b47df93a92b5","c0ba233a565243eca85255d150753d9a","333f1ff13fee4078aa7c0ae2d23ec7bb","0fd10dc272954f2e8aa2243dceec450d","3ecc05566e644e30a2d3ab8ef6608dde","91412103c98d4460b5591da405d1eb04","c663314a9c49436fbc6d7b02d56e25dc","cfeb9d8bb8444c1e90fe34e460d1c08f","5e3ebed1f68a4314a9c936df184e998f"]},"id":"BX3NkMPN2UjY","executionInfo":{"status":"ok","timestamp":1712415638134,"user_tz":-540,"elapsed":6435,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"ce1cac65-7305-4eb4-863c-d03710ae41de"},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b584cf4a35451282e117ee8ebe6c18"}},"metadata":{}}]},{"cell_type":"markdown","source":["### 2️⃣. HuggingFace에 Pipeline 으로 모델, 토크나이저 로드"],"metadata":{"id":"S04TobON2HLd"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, pipeline\n","import transformers\n","import torch\n","\n","model = \"taeseo06/Gemma-2b-PersonalizedFinetuning\" # https://huggingface.co/taeseo06/Gemma-2b-PersonalizedFinetuning\n","\n","\n","gemma_pipeline = pipeline(\n","    \"text-generation\",  # LLM task\n","    model=model,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"],"metadata":{"id":"jsBrtGpZYmcQ","colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["18a4db85e2234ec8b1fb88e499a7c2bf","a22f4cafca984284808ac48ffd99b241","51013517527a43e2bf06893a031b83fe","2c96fcb457db403cbf081d0e0eff464e","f79cfd0a60194188a8e04e1f0aa47e1f","8652175e344d41d3b40c0c93d6acf78c","7494f19ffeaa4fcb9dd1e27a3a3d529b","95a6928dd57848bca19c91eec92041c3","edd610756c324aff89df79bbabb56362","21fb50c1655149cfa131a928c18eca40","78ccf2c08f3e4ccfbb393802a5f4203f"]},"executionInfo":{"status":"ok","timestamp":1712416383097,"user_tz":-540,"elapsed":7689,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"97a0bbf0-0cde-4d47-badf-e04325096d8e"},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a4db85e2234ec8b1fb88e499a7c2bf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":[" ---\n"," ## Generate - 모델의 생성 함수"],"metadata":{"id":"LMQLbcLU4Jbt"}},{"cell_type":"code","source":["def get_response(prompt: str) -> None:\n","    \"\"\"\n","    Generate a response from the Llama model.\n","\n","    Parameters:\n","        prompt (str): The user's input/question for the model.\n","\n","    Returns:\n","        None: Prints the model's response.\n","    \"\"\"\n","    sequences = gemma_pipeline(\n","        prompt,\n","        do_sample=True,\n","        top_k=10,\n","        num_return_sequences=1,\n","        eos_token_id=tokenizer.eos_token_id,\n","        max_length=512,\n","    )\n","    print(\"Chatbot:\", sequences[0]['generated_text'])\n","\n"],"metadata":{"id":"bT9kSzLHhZ-d","executionInfo":{"status":"ok","timestamp":1712415480841,"user_tz":-540,"elapsed":571,"user":{"displayName":"문태서","userId":"03540612891035297255"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["get_response(\"안녕\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vPpgYifaRHB","executionInfo":{"status":"ok","timestamp":1712416301534,"user_tz":-540,"elapsed":17941,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"ef9abfd8-0fa4-430b-8878-7c6cf4036608"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Chatbot: 안녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕녕\n"]}]},{"cell_type":"code","source":["get_response(\"선린인터넷고등학교의 위치를 알려줘\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-yT4Pgt9z7F","executionInfo":{"status":"ok","timestamp":1712416361168,"user_tz":-540,"elapsed":15105,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"e84d75c3-43d2-438c-ced7-07fe13e1252f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Chatbot: 선린인터넷고등학교의 위치를 알려줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘줘\n"]}]},{"cell_type":"code","source":["# 텍스트 생성 함수 정의\n","def generate_text(prompt, max_length=512):\n","  # 토크나이징\n","  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n","\n","  # 모델 추론\n","  outputs = model.generate(input_ids=input_ids, max_length=max_length)\n","\n","  # 토큰을 텍스트로 변환\n","  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","  return generated_text\n","\n","# 예시\n","# prompt = \"문태서라는 인물에 관해서 설명해줘\"\n","# generated_text = generate_text(prompt)\n","# print(f\"Generated text: {generated_text}\")\n"],"metadata":{"id":"B0qrXBtzyu3T","executionInfo":{"status":"ok","timestamp":1712416388053,"user_tz":-540,"elapsed":570,"user":{"displayName":"문태서","userId":"03540612891035297255"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["prompt = \"안녕\"\n","generated_text = generate_text(prompt)\n","print(f\"Generated text: {generated_text}\")"],"metadata":{"id":"SUd5Pym3y63y","executionInfo":{"status":"ok","timestamp":1712415676080,"user_tz":-540,"elapsed":26852,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"f32b1994-417c-4266-c494-1252e71dba3d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated text: 안녕하세요!\n","\n","I'm looking for a fun and engaging way to learn Korean, and I've heard that watching Korean dramas is a great way to do that. But I'm not sure where to start.\n","\n","What are some good places to watch Korean dramas?\n","\n","I'd also like to know if there are any specific recommendations for beginner viewers.\n","\n","Thank you for your help!\n"]}]},{"cell_type":"code","source":["prompt = \"한국어로 답변할 수 있어?\"\n","generated_text = generate_text(prompt)\n","print(f\"Generated text: {generated_text}\")"],"metadata":{"id":"x7yItGcBzTSR","executionInfo":{"status":"ok","timestamp":1712415757319,"user_tz":-540,"elapsed":23284,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"1505836e-6652-4e3e-c7e7-ed212850c286","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated text: 한국어로 답변할 수 있어?\n","\n","Sure, here's the answer in Korean:\n","\n","Sure, here's the answer in Korean:\n","\n","**자신의 꿈과 목표를 정하고, 이를 달성하기 위해 어떻게 해야 할지 탐구해 보세요.**\n"]}]},{"cell_type":"markdown","source":["### Drawbacks of `get_response()`\n","\n","1. **Lack of Conversation History**: The basic approach does not account for past interactions, making it less effective for maintaining a coherent conversation.\n","2. **Limited Customization**: The function doesn't allow for advanced prompt formatting or handling system-level instructions.\n","3. **Not Ready for UI Integration**: This basic approach isn't designed for easy integration with user interface libraries like Gradio."],"metadata":{"id":"QD3Wr8F9oGjj"}},{"cell_type":"markdown","source":["## Improved Prompts\n","\n","The right structure of Llama 2 prompts:\n","\n","```\n","<s>[INST] <<SYS>>\n","{{ system_prompt }}\n","<</SYS>>\n","\n","{{ user_message }} [/INST]\n","```"],"metadata":{"id":"BbCnlXQ9iQIx"}},{"cell_type":"code","source":["# Gemma\n","\n","\n","'''\n","<bos><start_of_turn>user\n","Write a hello world program<end_of_turn>\n","<start_of_turn>model\n","'''"],"metadata":{"id":"sLq9Apkd9nTS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Building the Prompt\n","\n","Explaining the parameters:\n","- `message` is the current message we're sending\n","- `history` is the history of conversation as a list of tupples `[(user_msg1, bot_msg1), (usr_msg2, bot_msg2), ...]`\n"],"metadata":{"id":"0-J2F3zzWkcU"}},{"cell_type":"code","source":["SYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\n","You are a helpful bot. Your answers are clear and concise.\n","<</SYS>>\n","\n","\"\"\"\n","\n","SYSTEM_PROMPT = \"\"\"\n","\n","\"\"\"\n","\n","\n","\n","\n","# Formatting function for message and history\n","def format_message(message: str, history: list, memory_limit: int = 3) -> str:\n","    \"\"\"\n","    Formats the message and history for the Llama model.\n","\n","    Parameters:\n","        message (str): Current message to send.\n","        history (list): Past conversation history.\n","        memory_limit (int): Limit on how many past interactions to consider.\n","\n","    Returns:\n","        str: Formatted message string\n","    \"\"\"\n","    # always keep len(history) <= memory_limit\n","    if len(history) > memory_limit:\n","        history = history[-memory_limit:]\n","\n","    if len(history) == 0:\n","        return SYSTEM_PROMPT + f\"{message} [/INST]\"\n","\n","    formatted_message = SYSTEM_PROMPT + f\"{history[0][0]} [/INST] {history[0][1]} </s>\"\n","\n","    # Handle conversation history\n","    for user_msg, model_answer in history[1:]:\n","        formatted_message += f\"<s>[INST] {user_msg} [/INST] {model_answer} </s>\"\n","\n","    # Handle the current message\n","    formatted_message += f\"<s>[INST] {message} [/INST]\"\n","\n","    return formatted_message"],"metadata":{"id":"CiWBmmV6OWVw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Getting Responses\n","\n","We need the function to generate responses."],"metadata":{"id":"tlbsuGikcsB8"}},{"cell_type":"code","source":["# Generate a response from the Llama model\n","def get_llama_response(message: str, history: list) -> str:\n","    \"\"\"\n","    Generates a conversational response from the Llama model.\n","\n","    Parameters:\n","        message (str): User's input message.\n","        history (list): Past conversation history.\n","\n","    Returns:\n","        str: Generated response from the Llama model.\n","    \"\"\"\n","    query = format_message(message, history)\n","    response = \"\"\n","\n","    sequences = llama_pipeline(\n","        query,\n","        do_sample=True,\n","        top_k=10,\n","        num_return_sequences=1,\n","        eos_token_id=tokenizer.eos_token_id,\n","        max_length=1024,\n","    )\n","\n","    # generated_text = sequences[0]['generated_text']\n","    generated_text = generate_text(query)\n","    response = generated_text[len(query):]  # Remove the prompt from the output\n","\n","    print(\"Chatbot:\", response.strip())\n","    return response.strip()\n","\n","\n","\n","# message = \"안녕\"\n","\n","# query = format_message(message, [])\n","# response = \"\"\n","\n","# sequences = llama_pipeline(\n","#         query,\n","#         do_sample=True,\n","#         top_k=10,\n","#         num_return_sequences=1,\n","#         eos_token_id=tokenizer.eos_token_id,\n","#         max_length=1024,\n","#     )\n","\n","# # generated_text = sequences[0]['generated_text']\n","\n","# generated_text = generate_text(query)\n","# response = generated_text[len(query):]  # Remove the prompt from the output\n","\n","# print(\"Chatbot:\", response.strip())\n"],"metadata":{"id":"PeEh17FDLzEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response"],"metadata":{"id":"8RLLVslD2OVC","executionInfo":{"status":"ok","timestamp":1711625794233,"user_tz":-540,"elapsed":3,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"a0e5b8c9-5f4e-495b-8834-8e426132b143","colab":{"base_uri":"https://localhost:8080/","height":36}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nI am a helpful bot. I am here to assist you with any questions or tasks you may have. How can I help you today?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["response.strip()"],"metadata":{"id":"X_YVK7uV2CCR","executionInfo":{"status":"ok","timestamp":1711625787665,"user_tz":-540,"elapsed":2,"user":{"displayName":"문태서","userId":"03540612891035297255"}},"outputId":"04831185-e741-44f1-ff48-ea37a2c7caad","colab":{"base_uri":"https://localhost:8080/","height":36}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I am a helpful bot. I am here to assist you with any questions or tasks you may have. How can I help you today?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["import gradio as gr\n","\n","gr.ChatInterface(get_llama_response).launch(debug=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cHk0MVvdUF3S","outputId":"1c4228a2-85af-4218-cbff-147331342c0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://42dcfbe4df95c69f26.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://42dcfbe4df95c69f26.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Chatbot: 님! 어떻게 되세요?\n","\n","안녕하세요! 저는 어떻게 되세요? 질문해 주세요.\n","Chatbot: \n","Chatbot: \n","Chatbot: \n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 522, in process_events\n","    response = await route_utils.call_process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 260, in call_process_api\n","    output = await app.get_blocks().process_api(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1689, in process_api\n","    result = await self.call_function(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1253, in call_function\n","    prediction = await fn(*processed_input)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 717, in async_wrapper\n","    response = await f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 504, in _submit_fn\n","    response = await anyio.to_thread.run_sync(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n","    return await get_asynclib().run_sync_in_worker_thread(\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n","    return await future\n","  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n","    result = context.run(func, *args)\n","  File \"<ipython-input-34-0211a26be6e3>\", line 26, in get_llama_response\n","    generated_text = generate_text(query)\n","  File \"<ipython-input-15-b4342c4e011a>\", line 7, in generate_text\n","    outputs = model.generate(input_ids=input_ids, max_length=max_length)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1466, in generate\n","    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1186, in _validate_generated_length\n","    raise ValueError(\n","ValueError: Input length of input_ids is 121, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n"]}]},{"cell_type":"markdown","source":["### Conclusion\n","\n","Thanks to the Hugging Face Library, creating a pipeline to chat with llama 2 (or any other open-source LLM) is quite easy.\n","\n","But if you worked a lot with much larger models such as GPT-4, you need to adjust your expectations."],"metadata":{"id":"s3lxvhWKqfFd"}},{"cell_type":"code","source":[],"metadata":{"id":"CWD3HFWlr2BE"},"execution_count":null,"outputs":[]}]}