{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7pUj3c4uy4AlUW9XWb7/1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# 이 코랩 파일은 '학교 안내챗봇 만들기' 프로젝트 중 파인튜닝된 Gemma2b 모델을 gradio와 연결해서 챗봇 ui를 구성하는 부분입니다.\n","# 프로젝트 폴더 중 Test 폴더 안 여러 코랩 파일에서 개발 및 테스트를 거치고, 최종적으로 완성된 코드만 옮겨왔습니다."],"metadata":{"id":"V5-sSHgwIXo7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install gradio\n","!pip install accelerate\n","!pip install torch\n","!pip install python-dotenv"],"metadata":{"id":"l50d4VcVHBcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login # 허깅페이스 로그인\n","  # 개인 huggingface token을 입력"],"metadata":{"id":"z0iXyQNdHDPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdNDK5COGoTk"},"outputs":[],"source":["# 6/7 테스트 성공 (영어, 한국어)\n","\n","# import\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import re\n","import gradio as gr\n","\n","# 허깅페이스 API 토큰 지정\n","hf_token = '{huggingface-token}' # 이곳에 허깅페이스 토큰을 삽입\n","\n","# 모델 지정 및 데이터타입 설정 (16bit)\n","model_id = \"taeseo06/SchoolHelperChatbot-Gemma2B-Finetuning\" # model: https://huggingface.co/datasets/taeseo06/SchoolHelperChatbot-dataset\n","dtype = torch.bfloat16\n","\n","# 채팅 함수\n","def gemma_chat(message, history):\n","    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_id,\n","        use_auth_token=hf_token,\n","        torch_dtype=dtype,\n","    )\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    chat = [\n","        { \"role\": \"user\", \"content\": message },\n","    ]\n","\n","    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n","\n","    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(device)\n","    outputs = model.generate(input_ids=inputs, max_new_tokens=2048)\n","\n","    response = tokenizer.decode(outputs[0])\n","\n","    # 응답을 정리\n","    response_cleaned = re.split(\"model\", response)\n","\n","    # 응답 반환\n","    return response_cleaned[1]\n","\n","# Gradio 인터페이스 생성 및 실행\n","gr.ChatInterface(gemma_chat).launch(debug=True)"]}]}